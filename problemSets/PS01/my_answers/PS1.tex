\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 12, 2023}
\author{Lily Rice - 16304845 - Applied Stats II}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
	\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
\item This problem set is due before 23:59 on Sunday February 19, 2023. No late assignments will be accepted.
	\end{itemize}

	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq x) \frac{\sqrt {2\pi}}{x} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8x^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
	
\noindent As a hint, you can create the empirical distribution and theoretical CDF using this code:

\begin{lstlisting}[language=R]
	# create empirical distribution of observed data
	ECDF <- ecdf(data)
	empiricalCDF <- ECDF(data)
	# generate test statistic
	D <- max(abs(empiricalCDF - pnorm(data))) \end{lstlisting}

\vspace{1cm}

ANSWER:
First, I ran all code listed above. This was done as shown below:
\lstinputlisting[language=R, firstline=41, lastline=50]{PS1.R} 

Then, I estimated the p-value for this data using the ks.test() function in R. Jeff's equation for D uses the absolute value, which means we are conducting a two-tailed ks.test, which also means that rejection of the null hypothesis is determined by this p-value being lower than "alpha" (usually 0.05). I ran this ks.test with the following code:
	
\lstinputlisting[language=R, firstline=66, lastline=69]{PS1.R} 	

I was sure that this was the p-value I should be looking for when estimating it manually, as this test produced the correct D estimate (that matched the estimate from Jeff's test statistic equation).

This p-value of less than 2.22e-16 is extremely small, and definitely below the "default" significance level (0.05). This is grounds to reject our Null Hypothesis, that there is no difference between the reference function (the normal distribution) and the emperical cumulative distribution function (in other words, is grounds to reject the hypothesis that our data's CDF is normal).


I then attempted to estimate the p-value manually. I tried multiple different methods: including writing a new function, trying MLE to do so, pnorm(), and others, but was untimately unsuccessful at getting a function that produced a p-value that was appropriate. I have not included this code in this .pdf, however it is in the .R file if needed (and for my reference when correcting my work).
	




\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\vspace{.5cm}

I used the code listed to create my data first:

\lstinputlisting[language=R, firstline=123,lastline=125]{PS1.R} 

Then, I ran my OLS regression that uses the Quasi-Newton method on this data (first plotting my data just to have a look; then by coding the relationship between variables using R, and finally by running the regression on this coded linear likelihood equation). These steps can be seen in the code below:
\lstinputlisting[language=R, firstline=128,lastline=143]{PS1.R} 

This OLS regression returned the following parameters:
\lstinputlisting[language=R, firstline=145,lastline=146]{PS1.R} 

Then, I checked my estimates from the OLS regression by running a lm regression using the following code:
\lstinputlisting[language=R, firstline=149,lastline=149]{PS1.R} 
Which output the following summary statistics:
	\begin{verbatim}
Call:lm(formula = data$y ~ data$x)
Residuals:    
Min      1Q  Median      3Q     Max
 -3.1906 -0.9374 -0.1665  0.8931  4.8032
 
Coefficients:            
           Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.13919    0.25276   0.551    0.582    
data$x       2.72670    0.04159  65.564   <2e-16 ***---

Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.447 on 198 degrees of freedom
Multiple R-squared:  0.956,	Adjusted R-squared:  0.9557 
F-statistic:  4299 on 1 and 198 DF,  p-value: < 2.2e-16
\end{verbatim} 

This confirmed that my OLS regression using Quasi-Newton methods and my lm regression output equivalent results, as both output an intercept of 0.139 and a beta (slope) of 2.727. One difference between these two is that we don't get an estimate of sigma squared with the lm() method.


\end{document}
